# Summary of Text Processing and CSV File Generation Process for *Romeo and Juliet*  
The core objective of the document is to establish a complete workflow for *Romeo and Juliet*—from raw text cleaning to the generation of multiple CSV files—through program design and data processing, ultimately forming structured data usable for knowledge base construction. The specific steps and rules are as follows:  


## I. Raw Text Processing: Program Design and Text Preprocessing  
### 1. Program Modification Objective  
Modify the `process_romeo_juliet.py` program. Its core function is to perform **text cleaning and segmentation** on the raw text sourced from Project Gutenberg, laying the foundation for subsequent data extraction.  

### 2. Text Processing Content  
- **Cleaning Tasks**: Remove legal disclaimers, technical information, page numbers, and other technical markers from the text; delete redundant blank lines; and retain core plot text.  
- **Segmentation Tasks**: Segment the cleaned text by "Scene". Eventually, obtain content corresponding to the 307 "passages" in the original `collection.csv`, but uniformly name this content **core_text** (to emphasize that it is core content directly extracted from the raw text).  


## II. Core File Generation: `core.csv` (Core Knowledge Base Data)  
`core.csv` is the central file in the entire workflow. It records core content extracted from the raw text and corresponding question-answer pairs, containing 9 columns. It is divided into two categories: "basic records" and "inferred records".  

### 1. Column Structure Definition  
| Column Name               | Description                                                                 |
|---------------------------|-----------------------------------------------------------------------------|
| core_id                   | Unique identifier for core_text (format adjustment: "P039" in the original `generate_topics.py` is changed to "C039"). |
| core_text                 | Core content extracted from the raw text by Scene (corresponds to "passage" in the original `collection.csv`). |
| ACT_No                    | "Act" number to which the core_text belongs.                                |
| Scene                     | "Scene" number to which the core_text belongs.                              |
| core_Q_id                 | Unique identifier for the core question (core_Q).                           |
| core_Q                    | Core question generated based on core_text (referencing the topic logic in `generate_topics.py`). |
| core_A_id                 | Unique identifier for the core answer (core_A).                             |
| core_A                    | Answer corresponding to core_Q, generated based on core_text.               |
| relevance_judgment        | Relevance judgment (fixed value: 2 for basic records, 1 for inferred records). |

### 2. Generation Rules for the Two Types of Records  
#### (1) Basic Records (Known-type)  
- **Generation Logic**: One core_text can correspond to **multiple core_Qs (core questions)**. It is not necessary to generate questions for all core_texts—only approximately 40 Known-type questions need to be created.  
- **Example Reference**: For a core_text corresponding to "Nurse and Juliet in Capulet House (C039, Act I, Scene III)", a core_Q such as "How does Lady Capulet summon Juliet?" can be generated, and a corresponding core_A can be created based on this core_text.  

#### (2) Inferred Records  
- **Generation Logic**: Generated by synthesizing `core_A`s from multiple existing basic records. It must meet the requirement that "the question requires answers from multiple core_A s"—this step is relatively challenging.  
- **Field Concatenation Rules**:  
  - core_text: Concatenate core_texts from the basic records corresponding to multiple core_A s (e.g., core_A_id = 11, 23, 56).  
  - ACT_No/Scene: Concatenate ACT_No and Scene from the basic records corresponding to multiple core_A s.  
  - core_Q_id/core_A_id: Assign new unique identifiers.  
  - core_Q: Newly designed question that "requires collaboration of multiple core_A s to answer".  
  - core_A: Answer to the new question, synthesized from the information in the concatenated core_texts.  
  - relevance_judgment: Fixed at 1.  


## III. Derived File Generation: `topic.csv`, `groundtruth.csv`, `passage.csv`  
Downstream files are generated sequentially based on `core.csv`, with clear extraction and conversion rules for each step.  

### 1. `topic.csv` (Source of Topics and Variant Questions)  
- **Generation Steps**:  
  1. Extract the `core_Q_id` and `core_Q` columns from `core.csv`, and rename them to `topic_id` and `topic` respectively.  
  2. Supplement "questions not included in the knowledge base" (i.e., questions not based on any core_text) to complete the topic list.  
  3. Generate **multiple question variants** for each topic (referencing the variant logic in `generate_topics.py`).  
- **Hierarchical Structure**: Eventually form a hierarchical relationship of "1 core_text → multiple core_Qs (topics) → multiple question variants".  

### 2. `groundtruth.csv` (Ground Truth Comparison Data)  
- **Generation Steps**:  
  1. Extract 4 columns from `core.csv`: `core_Q_id`, `core_Q`, `core_A`, and `relevance_judgment`.  
  2. Rename `core_Q_id` to `topic_id` and `core_Q` to `topic`.  
  3. Exclude "questions not included in the knowledge base" (retain only records based on core_text).  
  4. Generate **multiple passage variants** for each `core_A`, and assign a unique `passage id` to each variant.  
  5. Delete the `core_A` column to obtain the final `groundtruth.csv`.  

### 3. `passage.csv` (Passage Data)  
- **Generation Logic**: Directly extract the `passage` (variants of core_A) and `passage id` columns from `groundtruth.csv`—no additional processing is required.  


## IV. Final Output: `gold_summaries` (Gold Standard Summaries)  
- **Generation Basis**: Generate `gold_summaries` by integrating information from the three previously generated files: `topic.csv` (topics), `passage.csv` (passages), and `core.csv` (core data). (The specific integration logic is not specified, but it must rely on the structured data of the three aforementioned files.)