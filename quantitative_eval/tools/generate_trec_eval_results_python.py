#!/usr/bin/env python3
"""
ä¸ºRomeo & Juliet RAGç³»ç»Ÿç”Ÿæˆtrec_evalè¯„ä¼°ç»“æœ - Pythonç‰ˆæœ¬
ä½¿ç”¨pytrec_evalåº“è¿›è¡Œè¯„ä¼°
"""

import os
import shutil
import pytrec_eval
from collections import defaultdict

def load_qrels(qrels_file: str):
    """
    åŠ è½½qrelsæ–‡ä»¶

    Args:
        qrels_file: qrelsæ–‡ä»¶è·¯å¾„

    Returns:
        dict: æŸ¥è¯¢ç›¸å…³æ€§åˆ¤æ–­å­—å…¸
    """
    qrels = defaultdict(dict)

    with open(qrels_file, 'r', encoding='utf-8') as f:
        for line in f:
            parts = line.strip().split()
            if len(parts) >= 4:
                query_id = parts[0]
                doc_id = parts[2]
                relevance = int(parts[3])
                qrels[query_id][doc_id] = relevance

    return dict(qrels)

def load_runs(runs_file: str):
    """
    åŠ è½½runsæ–‡ä»¶

    Args:
        runs_file: runsæ–‡ä»¶è·¯å¾„

    Returns:
        dict: æ£€ç´¢ç»“æœå­—å…¸
    """
    runs = defaultdict(dict)

    with open(runs_file, 'r', encoding='utf-8') as f:
        for line in f:
            parts = line.strip().split()
            if len(parts) >= 6:
                query_id = parts[0]
                doc_id = parts[2]
                score = float(parts[4])
                runs[query_id][doc_id] = score

    return dict(runs)

def run_pytrec_eval(qrels_file: str, runs_file: str, output_file: str):
    """
    ä½¿ç”¨pytrec_evalè¿›è¡Œè¯„ä¼°

    Args:
        qrels_file: ç›¸å…³æ€§åˆ¤æ–­æ–‡ä»¶è·¯å¾„
        runs_file: æ£€ç´¢ç»“æœæ–‡ä»¶è·¯å¾„
        output_file: è¾“å‡ºç»“æœæ–‡ä»¶è·¯å¾„

    Returns:
        bool: æ˜¯å¦æˆåŠŸ
    """
    try:
        print(f"åŠ è½½qrels: {qrels_file}")
        qrels = load_qrels(qrels_file)
        print(f"æŸ¥è¯¢æ•°é‡: {len(qrels)}")

        print(f"åŠ è½½runs: {runs_file}")
        runs = load_runs(runs_file)
        print(f"æŸ¥è¯¢æ•°é‡: {len(runs)}")

        # åˆ›å»ºè¯„ä¼°å™¨ - ä½¿ç”¨P@4å’ŒRecall@4æ›´ç¬¦åˆå®é™…ï¼ˆæ¯ä¸ªtopicå¯¹åº”4ä¸ªæ®µè½ï¼‰
        evaluator = pytrec_eval.RelevanceEvaluator(
            qrels,
            {'map', 'ndcg', 'P_4', 'P_10', 'recall_4', 'recall_10', 'bpref'}
        )

        # è¿è¡Œè¯„ä¼°
        results = evaluator.evaluate(runs)

        # æ ¼å¼åŒ–è¾“å‡º
        output_lines = []

        # è®¡ç®—æ€»ä½“å¹³å‡å€¼
        all_metrics = defaultdict(list)
        for query_id, query_results in results.items():
            for metric, value in query_results.items():
                all_metrics[metric].append(value)

        # å…ˆè¾“å‡ºæ¯ä¸ªæŸ¥è¯¢çš„ç»“æœ
        for query_id in sorted(results.keys()):
            query_results = results[query_id]
            for metric in sorted(query_results.keys()):
                value = query_results[metric]
                output_lines.append(f"{metric:<20}\t{query_id}\t{value:.4f}")

        # è¾“å‡ºæ€»ä½“å¹³å‡å€¼
        output_lines.append("")  # ç©ºè¡Œåˆ†éš”
        for metric in sorted(all_metrics.keys()):
            values = all_metrics[metric]
            avg_value = sum(values) / len(values) if values else 0.0
            output_lines.append(f"{metric:<20}\tall\t{avg_value:.4f}")

        # ä¿å­˜ç»“æœ
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(output_lines))

        print(f"è¯„ä¼°ç»“æœä¿å­˜åˆ°: {output_file}")

        # æ˜¾ç¤ºå…³é”®æŒ‡æ ‡
        print("å…³é”®æŒ‡æ ‡ (å¹³å‡å€¼):")
        key_metrics = ['map', 'ndcg', 'P_4', 'recall_4', 'bpref']
        for metric in key_metrics:
            if metric in all_metrics:
                values = all_metrics[metric]
                avg_value = sum(values) / len(values)
                print(f"  {metric.upper()}: {avg_value:.4f}")

        return True

    except Exception as e:
        print(f"è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        return False

def generate_tex_summary(txt_file: str, tex_file: str, method_name: str):
    """
    ç”ŸæˆLaTeXæ ¼å¼çš„è¯„ä¼°æ‘˜è¦

    Args:
        txt_file: trec_evalæ–‡æœ¬ç»“æœæ–‡ä»¶
        tex_file: è¾“å‡ºLaTeXæ–‡ä»¶è·¯å¾„
        method_name: æ–¹æ³•åç§°
    """
    if not os.path.exists(txt_file):
        print(f"âŒ æ–‡æœ¬ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {txt_file}")
        return

    # è¯»å–å¹³å‡å€¼æŒ‡æ ‡
    metrics = {}

    with open(txt_file, 'r', encoding='utf-8') as f:
        for line in f:
            parts = line.strip().split('\t')
            if len(parts) >= 3 and parts[1] == "all":
                metric_name = parts[0].strip()
                value = parts[2]
                metrics[metric_name] = value

    # ç”ŸæˆLaTeXè¡¨æ ¼
    latex_content = f"""% Romeo & Juliet RAG System - {method_name} è¯„ä¼°ç»“æœ
\\begin{{table}}[h]
\\centering
\\caption{{{method_name} æ–¹æ³•è¯„ä¼°ç»“æœ}}
\\begin{{tabular}}{{|l|c|}}
\\hline
æŒ‡æ ‡ & å€¼ \\\\
\\hline
"""

    # æ·»åŠ ä¸»è¦æŒ‡æ ‡ï¼ˆä½¿ç”¨P@4å’ŒRecall@4æ›´ç¬¦åˆå®é™…ï¼‰
    key_metrics = {
        'map': 'MAP',
        'P_4': 'P@4',
        'P_10': 'P@10',
        'recall_4': 'Recall@4',
        'recall_10': 'Recall@10',
        'ndcg': 'NDCG',
        'bpref': 'BPref'
    }

    for metric_key, metric_label in key_metrics.items():
        if metric_key in metrics:
            latex_content += f"{metric_label} & {metrics[metric_key]} \\\\\n"

    latex_content += """\\hline
\\end{tabular}
\\end{table}
"""

    # ä¿å­˜LaTeXæ–‡ä»¶
    os.makedirs(os.path.dirname(tex_file), exist_ok=True)
    with open(tex_file, 'w', encoding='utf-8') as f:
        f.write(latex_content)

    print(f"LaTeXæ‘˜è¦ä¿å­˜åˆ°: {tex_file}")

def clear_old_results():
    """æ¸…ç†æ—§çš„RMITç›¸å…³è¯„ä¼°ç»“æœ"""
    results_dir = "target/trec_eval_results"

    if not os.path.exists(results_dir):
        print("trec_eval_resultsç›®å½•ä¸å­˜åœ¨ï¼Œè·³è¿‡æ¸…ç†")
        return

    print("=== æ¸…ç†æ—§çš„RMITè¯„ä¼°ç»“æœ ===")

    # å¤‡ä»½æ—§æ–‡ä»¶
    backup_dir = "target/trec_eval_results_backup_rmit"
    if os.path.exists(results_dir):
        if os.path.exists(backup_dir):
            shutil.rmtree(backup_dir)
        shutil.copytree(results_dir, backup_dir)
        print(f"æ—§æ–‡ä»¶å·²å¤‡ä»½åˆ°: {backup_dir}")

    # æ¸…ç©ºç›®å½•
    if os.path.exists(results_dir):
        shutil.rmtree(results_dir)
    os.makedirs(results_dir, exist_ok=True)

    print("æ—§è¯„ä¼°ç»“æœå·²æ¸…ç†")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ­ ç”ŸæˆRomeo & Juliet RAGç³»ç»Ÿtrec_evalè¯„ä¼°ç»“æœ (Pythonç‰ˆ)")
    print("=" * 65)

    # ç¡®ä¿åœ¨æ­£ç¡®çš„ç›®å½•ä¸­
    if not os.path.exists("data/qrels.txt"):
        print("âŒ è¯·åœ¨quantitative_evalç›®å½•ä¸­è¿è¡Œæ­¤è„šæœ¬")
        return

    # æ¸…ç†æ—§ç»“æœ
    clear_old_results()

    # æ–‡ä»¶è·¯å¾„é…ç½®
    qrels_file = "data/qrels.txt"
    runs_configs = [
        {
            "runs_file": "target/runs/romeo-juliet-faiss.txt",
            "output_txt": "target/trec_eval_results/romeo-juliet-faiss.txt",
            "output_tex": "target/trec_eval_results/romeo-juliet-faiss.tex",
            "method_name": "FAISS Dense Retrieval"
        },
        {
            "runs_file": "target/runs/romeo-juliet-bm25.txt",
            "output_txt": "target/trec_eval_results/romeo-juliet-bm25.txt",
            "output_tex": "target/trec_eval_results/romeo-juliet-bm25.tex",
            "method_name": "BM25 Keyword Search"
        },
        {
            "runs_file": "target/runs/romeo-juliet-intent.txt",
            "output_txt": "target/trec_eval_results/romeo-juliet-intent.txt",
            "output_tex": "target/trec_eval_results/romeo-juliet-intent.tex",
            "method_name": "Intent-based Retrieval"
        }
    ]

    success_count = 0

    for config in runs_configs:
        runs_file = config["runs_file"]
        output_txt = config["output_txt"]
        output_tex = config["output_tex"]
        method_name = config["method_name"]

        print(f"\n=== å¤„ç† {method_name} ===")

        if not os.path.exists(runs_file):
            print(f"âš ï¸  runsæ–‡ä»¶ä¸å­˜åœ¨: {runs_file}")
            continue

        # ç”Ÿæˆtrec_evalç»“æœ
        if run_pytrec_eval(qrels_file, runs_file, output_txt):
            # ç”ŸæˆLaTeXæ‘˜è¦
            generate_tex_summary(output_txt, output_tex, method_name)
            success_count += 1
        else:
            print(f"âŒ {method_name} è¯„ä¼°å¤±è´¥")

    print(f"\nâœ… å®Œæˆï¼æˆåŠŸç”Ÿæˆäº† {success_count}/{len(runs_configs)} ä¸ªè¯„ä¼°ç»“æœ")

    # æ˜¾ç¤ºç»“æœæ–‡ä»¶
    results_dir = "target/trec_eval_results"
    if os.path.exists(results_dir):
        print(f"\nğŸ“Š ç”Ÿæˆçš„è¯„ä¼°ç»“æœæ–‡ä»¶:")
        for filename in sorted(os.listdir(results_dir)):
            filepath = os.path.join(results_dir, filename)
            if os.path.isfile(filepath):
                file_size = os.path.getsize(filepath)
                print(f"  {filename}: {file_size} bytes")

if __name__ == "__main__":
    main()